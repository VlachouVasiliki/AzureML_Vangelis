{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "https://prrao87.github.io/blog/spacy/nlp/performance/2020/05/02/spacy-multiprocess.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Uncommend and run the following pip & python commands when running a new compute for the <b> first</b> time! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1616410189884
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#!python -m spacy download el_core_news_sm\n",
    "#!pip install pyarrow --upgrade\n",
    "#!pip install openpyxl\n",
    "#!pip install xlrd\n",
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1616410189990
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "#import el_core_news_sm\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace, Dataset\n",
    "from spacy.cli.download import download as spacy_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "gather": {
     "logged": 1616410190398
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#lemmatizer = GreekLemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('el_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy_download('el_core_news_sm')\n",
    "nlp =spacy.load('el_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1616410190501
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "p1 = re.compile('δεν απαντ.{1,3}\\s{0,1}',re.IGNORECASE)\n",
    "p2 = re.compile('\\sδα\\s',re.IGNORECASE)\n",
    "p3 = re.compile('δε.{0,1}\\s.{0,3}\\s{0,1}βρ.{1,2}κ.\\s{0,1}',re.IGNORECASE)\n",
    "p4 = re.compile('[^\\d]?\\d{10}')\n",
    "p5 = re.compile('[^\\d]?\\d{18}|[^\\d]\\d{20}')\n",
    "p6 = re.compile('δε[ ν]{0,1} (επιθυμ[α-ω]{2,4}?|[ήη]θ[εέ]λ[α-ω]{1,3}?|θελ[α-ω]{1,4}|.{0,20}ενδιαφ[εέ]ρ[α-ω]{2,4})',re.IGNORECASE)\n",
    "p7 = re.compile('δε[ ν]{0,1} (μπορ[α-ω]{2,5}|.εχει)',re.IGNORECASE)\n",
    "p8 = re.compile('(δεν|μη).*διαθεσιμ[οη]ς{0,1}?',re.IGNORECASE)\n",
    "p9 = re.compile('(δεν|μη)+.*εφικτη?',re.IGNORECASE)\n",
    "p10 = re.compile('δε[ ν]{0,1}.{0,20}θετικ[οόήη]ς{0,1}',re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "gather": {
     "logged": 1616410190614
    }
   },
   "outputs": [],
   "source": [
    "def loadStopWords():\n",
    "    sWords = open('stopWords.txt','r',encoding='utf-8')\n",
    "    sw = set(sWords.read().split('\\n'))\n",
    "    #sw = sw.remove('μη')\n",
    "    sWords.close()\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1616410190728
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def replaceTerm(text):\n",
    "    text = p5.sub(' λογαριασμός ',text)\n",
    "    text = p4.sub(' τηλεφωνο ',text)\n",
    "    text = p6.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p10.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p7.sub(' δενεχειδενμπορει ',text)\n",
    "    text = p8.sub(' δενειναιδιαθεσιμος ',text)\n",
    "    text = p9.sub(' ανεφικτη ',text)\n",
    "    text = text.replace('-banking','banking')\n",
    "    text = text.replace('v banking','vbanking')\n",
    "    text = text.replace('e banking','ebanking')\n",
    "    text = text.replace('follow up','followup')\n",
    "    text = text.replace('fup','followup')\n",
    "    text = text.replace('f/up','followup')\n",
    "    text = text.replace('πυρ/ριο','πυρασφαλιστηριο')\n",
    "    text = text.replace('safe drive','safedrive')\n",
    "    text = text.replace('safe pocket','safepocket')\n",
    "    text = text.replace('alphabank','alpha')\n",
    "    text = text.replace('sweet home smart','sweethomesmart')\n",
    "    text = text.replace('sweet home','sweethome')\n",
    "    text = text.replace('eξασφαλιζω','εξασφαλιζω')\n",
    "    text = text.replace('credit card','creditcard')\n",
    "    text = text.replace('debit card','debitcard')\n",
    "    text = text.replace('life cycle','lifecycle')\n",
    "    text = text.replace('π/κ','πκ')\n",
    "    text = text.replace('td','πκ')\n",
    "    text = text.replace('α/κ','ακ')\n",
    "    text = text.replace('δ/α','δεναπαντα ')\n",
    "    text = text.replace('εκτος αττικης','εκτοςαττικης ')\n",
    "    #τδ\n",
    "    text = p1.sub(' δεναπαντα ',text)\n",
    "    text = p2.sub(' δεναπαντα ',text)\n",
    "    text = p3.sub(' δεντονβρηκα ',text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1616410190919
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#sw = nlp.Defaults.stop_words\n",
    "#sw = sw|{'εχω','απο','ωστε'}\n",
    "sw = loadStopWords()\n",
    "def remove_ton(text):\n",
    "    diction = {'ά':'α','έ':'ε','ί':'ι','ό':'ο','ώ':'ω','ύ':'υ'}\n",
    "    for key in diction.keys():\n",
    "        text = text.replace(key, diction[key])\n",
    "    return text   \n",
    "def clean_text(text):\n",
    "     #text to string\n",
    "    text = str(text).lower()\n",
    "    text = replaceTerm(text)\n",
    "    \n",
    "   # tokenize text and remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    # lower text\n",
    "    text = [remove_ton(x) for x in text]\n",
    "    # remove stop words\n",
    "    text = [x for x in text if x not in sw]\n",
    " \n",
    "    #remove quotes\n",
    "    text = [x.replace('quot;','').replace('&quot','') for x in text if x not in {'quot','amp'}]\n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)]\n",
    "    # remove empty tokens\n",
    "    text = [t for t in text if len(t) > 0]\n",
    "    # remove amp & quot\n",
    "    text = [x for x in text if x not in ['quot','amp']]\n",
    "    # remove words with only one letter\n",
    "    text = \" \".join([t for t in text if len(t) > 1])\n",
    "     # lemmatize text\n",
    "    text = \" \".join([t.lemma_ for t in nlp(text, disable=['tagger', 'parser', 'ner','tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler',  'ner'])])\n",
    "   \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1616410191247
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def correct(x,corDict):\n",
    "    if x in corDict.keys():\n",
    "        y = corDict[x]\n",
    "    else:\n",
    "        y = x\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1616410191356
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def get_ngrams(idf,mindf,minngram,maxngram):\n",
    "    tfidf = TfidfVectorizer(min_df = mindf,ngram_range = (minngram,maxngram))\n",
    "    tfidf_result = tfidf.fit_transform(idf['tokenized']).toarray()\n",
    "    tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
    "    tfidf_df.columns = [str(x) for x in tfidf_df.columns]\n",
    "    df_i = pd.concat([df[['CON_ROW_ID']],tfidf_df],axis=1).melt(id_vars=['CON_ROW_ID'],value_vars = tfidf_df.columns).dropna()\n",
    "    df_i = df_i[df_i['value']>0]\n",
    "    return df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "gather": {
     "logged": 1616410191467
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def cleanComments(df):\n",
    "    df = df[['CON_ROW_ID','CON_COMMENTS']]\n",
    "    df['tokenized'] = df['CON_COMMENTS'].apply(clean_text)\n",
    "    df = df.fillna('N/A')\n",
    "    df['variable'] = df['tokenized'].str.split()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "gather": {
     "logged": 1616410191572
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def getTokens(df):\n",
    "    df = cleanComments(df)\n",
    "    df_f = df.explode('variable')[['CON_ROW_ID','variable']]\n",
    "    return df_f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "gather": {
     "logged": 1616410811261
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "def getTokencount(df_f,minCount):\n",
    "    tokenCount = df_f['variable'].value_counts().to_dict()\n",
    "    df_f['value'] = df_f['variable'].map(tokenCount)\n",
    "    df_f=df_f[df_f['value']>minCount] \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1616411290139
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "txt = 'AYJHSE SXESH? POYLHSE AKINHTO?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ayjhse', 'sxesh', '?', 'poylhse', 'akinhto', '?']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.lemma_ for t in nlp(txt, disable=['tagger', 'parser', 'ner','tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler',  'ner'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "gather": {
     "logged": 1616411292618
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "com = {'CON_ROW_ID':[1],'CON_COMMENTS':[txt]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "gather": {
     "logged": 1616411295066
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(com)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1616411297325
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "gather": {
     "logged": 1616411301738
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df = cleanComments(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1616411304474
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "gather": {
     "logged": 1616411321365
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py36/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_f = getTokens(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "gather": {
     "logged": 1616411326724
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "gather": {
     "logged": 1616411332866
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "tokenCount = df_f['variable'].value_counts().to_dict()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "gather": {
     "logged": 1616411338037
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "gather": {
     "logged": 1616411349517
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_f['value'] = df_f['variable'].map(tokenCount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "gather": {
     "logged": 1616411354397
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "gather": {
     "logged": 1616411370625
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_f=df_f[df_f['value']>0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "gather": {
     "logged": 1616411374016
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "gather": {
     "logged": 1616411384706
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "corDict = dict(pd.read_excel(\"corTokens.xls\").to_dict(\"split\")['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "gather": {
     "logged": 1616411388883
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_f['token'] = df_f['variable'].apply(lambda x : correct(x,corDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "gather": {
     "logged": 1616411392388
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "gather": {
     "logged": 1616411400138
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_f = df_f[df_f['token'] !='rmv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "gather": {
     "logged": 1616411402734
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "gather": {
     "logged": 1616411420001
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_f = df_f[df_f['token'].str.len() >1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "gather": {
     "logged": 1616411423123
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "gather": {
     "logged": 1616411427695
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_f = df_f.fillna('N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "gather": {
     "logged": 1616411432077
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#df_f.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "gather": {
     "logged": 1616411436403
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_f = df_f[['CON_ROW_ID','token']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "gather": {
     "logged": 1616411439920
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ayjhse\n",
       "0      sxesh\n",
       "0    poylhse\n",
       "0    akinhto\n",
       "Name: token, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_f['token'].head(10)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
