{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<b>Import necessery libraries</b>\n"
      ],
      "metadata": {},
      "id": "qualified-english"
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from azureml.core import Experiment\n",
        "from azureml.core import Workspace, Dataset\n",
        "from azureml.data import DataType\n",
        "from spacy.cli.download import download as spacy_download\n",
        "import os \n",
        "from os.path import join as osjoin"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1639565695938
        }
      },
      "id": "advised-contact"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Select the default workspace & datastore</b>"
      ],
      "metadata": {},
      "id": "worse-franchise"
    },
    {
      "cell_type": "code",
      "source": [
        "# azureml-core of version 1.0.72 or higher is required\n",
        "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "subscription_id = '6ed9d167-b2e6-41b8-9500-35e6df64d9dc'\n",
        "resource_group = 'MLRG'\n",
        "workspace_name = 'erbbimlws'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "\n",
        "datastore = workspace.get_default_datastore()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "lesbian-teddy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Loading the Greek language tools</b>"
      ],
      "metadata": {},
      "id": "stunning-timing"
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_download('el_core_news_sm')\n",
        "nlp =spacy.load('el_core_news_sm', disable=['tagger', 'parser', 'ner'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "clear-segment"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Parameter definitions</b>"
      ],
      "metadata": {},
      "id": "coated-alarm"
    },
    {
      "cell_type": "code",
      "source": [
        "#minimum number of tokens in the texts\n",
        "minCount = 1\n",
        "#ngrams parameters\n",
        "mindf,minngram,maxngram = 30,2,3\n",
        "#keep empty tokens\n",
        "deleteEmptyTokens = True\n",
        "#dataset name to be analyzed\n",
        "datasetName = 'LinkComments202108'\n",
        "#export filename\n",
        "fileName = 'LinkComments202108_exp'"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "documentary-cleaners"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Regular expressions definitions</b>"
      ],
      "metadata": {},
      "id": "thorough-basics"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "p1 = re.compile('δεν απαντ.{1,3}\\s{0,1}',re.IGNORECASE)\n",
        "p2 = re.compile('\\sδα\\s',re.IGNORECASE)\n",
        "p3 = re.compile('δε.{0,1}\\s.{0,3}\\s{0,1}βρ.{1,2}κ.\\s{0,1}',re.IGNORECASE)\n",
        "p4 = re.compile('[^\\d]?\\d{10}')\n",
        "p5 = re.compile('[^\\d]?\\d{18}|[^\\d]\\d{20}')\n",
        "p6 = re.compile('δε[ ν]{0,1} (επιθυμ[α-ω]{2,4}?|[ήη]θ[εέ]λ[α-ω]{1,3}?|θελ[α-ω]{1,4}|.{0,20}ενδιαφ[εέ]ρ[α-ω]{2,4})',re.IGNORECASE)\n",
        "p7 = re.compile('δε[ ν]{0,1} (μπορ[α-ω]{2,5}|.εχει)',re.IGNORECASE)\n",
        "p8 = re.compile('(δεν|μη).*διαθεσιμ[οη]ς{0,1}?',re.IGNORECASE)\n",
        "p9 = re.compile('(δεν|μη)+.*εφικτη?',re.IGNORECASE)\n",
        "p10 = re.compile('δε[ ν]{0,1}.{0,20}θετικ[οόήη]ς{0,1}',re.IGNORECASE)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "greenhouse-chicago"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Functions definitions</b>"
      ],
      "metadata": {},
      "id": "atmospheric-bobby"
    },
    {
      "cell_type": "code",
      "source": [
        "def loadStopWords(ws):\n",
        "    #A dataset containing the Greek stop words has been created\n",
        "    #the function loads this dataset as a dataframe\n",
        "    dataset = Dataset.get_by_name(ws, name='stopWords_gr')\n",
        "    sw = set(dataset.to_pandas_dataframe())\n",
        "    return sw"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "permanent-tolerance"
    },
    {
      "cell_type": "code",
      "source": [
        "def replaceTerm(text):\n",
        "    #This function uses the above defined regular expressions to replace text\n",
        "    #The order of the rules is importand\n",
        "    #Compinations of two or more words, are concatenated, in order to be considered as a single token\n",
        "    text = p5.sub(' λογαριασμός ',text)\n",
        "    text = p4.sub(' τηλεφωνο ',text)\n",
        "    text = p6.sub(' δενθελειδενενδιαφερεται ',text)\n",
        "    text = p10.sub(' δενθελειδενενδιαφερεται ',text)\n",
        "    text = p7.sub(' δενεχειδενμπορει ',text)\n",
        "    text = p8.sub(' δενειναιδιαθεσιμος ',text)\n",
        "    text = p9.sub(' ανεφικτη ',text)\n",
        "    text = text.replace('-banking','banking')\n",
        "    text = text.replace('v banking','vbanking')\n",
        "    text = text.replace('e banking','ebanking')\n",
        "    text = text.replace('follow up','followup')\n",
        "    text = text.replace('fup','followup')\n",
        "    text = text.replace('f/up','followup')\n",
        "    text = text.replace('πυρ/ριο','πυρασφαλιστηριο')\n",
        "    text = text.replace('safe drive','safedrive')\n",
        "    text = text.replace('safe pocket','safepocket')\n",
        "    text = text.replace('alphabank','alpha')\n",
        "    text = text.replace('sweet home smart','sweethomesmart')\n",
        "    text = text.replace('sweet home','sweethome')\n",
        "    text = text.replace('eξασφαλιζω','εξασφαλιζω')\n",
        "    text = text.replace('credit card','creditcard')\n",
        "    text = text.replace('debit card','debitcard')\n",
        "    text = text.replace('life cycle','lifecycle')\n",
        "    text = text.replace('π/κ','πκ')\n",
        "    text = text.replace('td','πκ')\n",
        "    text = text.replace('α/κ','ακ')\n",
        "    text = text.replace('δ/α','δεναπαντα ')\n",
        "    text = text.replace('εκτος αττικης','εκτοςαττικης ')\n",
        "    #τδ\n",
        "    text = p1.sub(' δεναπαντα ',text)\n",
        "    text = p2.sub(' δεναπαντα ',text)\n",
        "    text = p3.sub(' δεντονβρηκα ',text)\n",
        "    \n",
        "    return text"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "documentary-nelson"
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_ton(text):\n",
        "    #removes punctuation, αφαιρεί τους τόνους\n",
        "    diction = {'ά':'α','έ':'ε','ί':'ι','ό':'ο','ώ':'ω','ύ':'υ','ή':'η'}\n",
        "    for key in diction.keys():\n",
        "        text = text.replace(key, diction[key])\n",
        "    return text   "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "static-crossing"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text,sw):\n",
        "    #This function performs text cleansing and returns the clean and lemmatized version of the original text\n",
        "    #conver to lower text \n",
        "    text = str(text).lower()\n",
        "    #replacements either by rules or regular expressions\n",
        "    text = replaceTerm(text)\n",
        "    \n",
        "   # remove puncutation\n",
        "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
        "    \n",
        "    # αφαιρούνται οι τόνοι\n",
        "    text = [remove_ton(x) for x in text]\n",
        "    \n",
        "    # remove stop words\n",
        "    text = [x for x in text if x not in sw]\n",
        "\n",
        "    #remove quotes\n",
        "    text = [x.replace('quot;','').replace('&quot','') for x in text if x not in {'quot','amp'}]\n",
        "    \n",
        "    # remove words that contain numbers\n",
        "    text = [word for word in text if not any(c.isdigit() for c in word)] #addition to return even empty tokens\n",
        "    \n",
        "    # remove empty tokens\n",
        "    #text = [t for t in text if len(t) > 0] #addition to return even empty tokens\n",
        "    \n",
        "    # remove amp & quot\n",
        "    text = [x for x in text if x not in ['quot','amp']]\n",
        "    \n",
        "    # remove words with only one letter\n",
        "    text = \" \".join([t for t in text if len(t) > -1]) #addition to return even empty tokens\n",
        "    \n",
        "    # lemmatize text\n",
        "    text = \" \".join([t.lemma_ for t in nlp(text, disable=['tagger', 'parser', 'ner','tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler',  'ner'])])\n",
        "    \n",
        "    return(text)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "painful-dairy"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_correctDict(ws):\n",
        "    #it creates a dictionary out of a dataset that containes pairs of (original term, corrected term)    \n",
        "    dataset = Dataset.get_by_name(ws, name='correct_Tokens')    \n",
        "    corDict = dict(dataset.to_pandas_dataframe().to_dict(\"split\")['data'])\n",
        "    return corDict"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "spoken-novel"
    },
    {
      "cell_type": "code",
      "source": [
        "def correct(x,corDict):\n",
        "    #uses the dictionary to correct the terms\n",
        "    if x in corDict.keys():\n",
        "        y = corDict[x]\n",
        "    else:\n",
        "        y = x\n",
        "    return y    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "passing-suite"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(idf,mindf,minngram,maxngram):\n",
        "    #this function returns the bi-grams and tri-grams\n",
        "    tfidf = TfidfVectorizer(min_df = mindf,ngram_range = (minngram,maxngram))\n",
        "    tfidf_result = tfidf.fit_transform(idf['tokenized']).toarray()\n",
        "    tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n",
        "    tfidf_df.columns = [str(x) for x in tfidf_df.columns]\n",
        "    df_i = pd.concat([df[['CON_ROW_ID']],tfidf_df],axis=1).melt(id_vars=['CON_ROW_ID'],value_vars = tfidf_df.columns).dropna()\n",
        "    df_i = df_i[df_i['value']>0]\n",
        "    return df_i"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "global-chess"
    },
    {
      "cell_type": "code",
      "source": [
        "def cleanComments(df,sw):\n",
        "    #applies the clean text function to all texts contained in the dataset\n",
        "    df = df[['CON_ROW_ID','CON_COMMENTS']]\n",
        "    df['tokenized'] = df['CON_COMMENTS'].apply(clean_text,args = (sw))\n",
        "    df = df.fillna('N/A')\n",
        "    df['variable'] = df['tokenized'].str.split()\n",
        "    return df"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "environmental-pointer"
    },
    {
      "cell_type": "code",
      "source": [
        "def getTokens(df,sw):\n",
        "    #The variable columns is a list. The explode method \"unpivots this list\"\n",
        "    df = cleanComments(df,sw)\n",
        "    df_f = df.explode('variable')[['CON_ROW_ID','variable']]\n",
        "    return df_f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "improving-smoke"
    },
    {
      "cell_type": "code",
      "source": [
        "def getTokencount(df_f,minCount):\n",
        "    #calculate the number of occurances (counts) of each token\n",
        "    #tokens with count less than mincount are set to blank\n",
        "    tokenCount = df_f['variable'].value_counts().to_dict()\n",
        "    \n",
        "    df_f['value'] = df_f['variable'].map(tokenCount)\n",
        "   \n",
        "    df_f.loc[(df_f['value']<minCount), 'variable'] = ' ' #addition to return even empty tokens\n",
        "    \n",
        "    return df_f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "personal-hands"
    },
    {
      "cell_type": "code",
      "source": [
        "def performNLP(workspace,minCount,mindf,minngram,maxngram,deleteEmptyTokens,df):\n",
        "    sw = loadStopWords(workspace)\n",
        "    \n",
        "    #df = cleanComments(df,sw)\n",
        "    \n",
        "    df_f = getTokens(df,sw)\n",
        "    \n",
        "    df_f = df_f.fillna(' ')\n",
        "    \n",
        "    df_f = getTokencount(df_f,minCount)\n",
        "    \n",
        "    try:\n",
        "        df_f = df_f.append(get_ngrams(df,mindf,minngram,maxngram ))\n",
        "    except:\n",
        "        print('no bigramms or trigramms were added')\n",
        "    \n",
        "    corDict = load_correctDict(workspace)     \n",
        "    \n",
        "    df_f['token'] = df_f['variable'].apply(lambda x : correct(x,corDict))\n",
        "    \n",
        "    df_f.loc[(df_f['token'].str.len() <2), 'token'] = ' ' #single character tokens are set to blank\n",
        "    \n",
        "    df_f = df_f.sort_values(['CON_ROW_ID','token'])\n",
        "    \n",
        "    if deleteEmptyTokens:\n",
        "        df_f = df_f[df_f['token'] != ' ']\n",
        "    \n",
        "    df_f = df_f[['CON_ROW_ID','token']].drop_duplicates()\n",
        "    \n",
        "    return df_f"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "bizarre-finger"
    },
    {
      "cell_type": "code",
      "source": [
        "def loadTexts(workspace,datasetName):\n",
        "    #loads the texts to be analyzed\n",
        "    dataset = Dataset.get_by_name(workspace, name=datasetName)\n",
        "    df = dataset.to_pandas_dataframe()\n",
        "    df= df[['CON_ROW_ID','CON_COMMENTS']]\n",
        "    return df   "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "synthetic-density"
    },
    {
      "cell_type": "code",
      "source": [
        "def exportResults(workspace,datastore,fileName,df_f):\n",
        "    df_f.to_csv(fileName+'.txt',sep =',',line_terminator='\\r\\n',index = False)\n",
        "    fil = [os.getcwd()+'/'+ fileName+'.txt']\n",
        "    datastore.upload_files(fil, target_path='UI/NLP', overwrite=True, show_progress=True)\n",
        "    \n",
        "    "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "intimate-architect"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>The commended-out code is for debuging purposes</b>"
      ],
      "metadata": {},
      "id": "biological-channels"
    },
    {
      "cell_type": "code",
      "source": [
        "txt = 'H eurobank είναι καλύτερη τράπεζα στον κόσμο. Η εμπειρία του καταστήματος είναι τραγική'\n",
        "com = {'CON_ROW_ID':[1],'CON_COMMENTS':[txt]}\n",
        "df = pd.DataFrame(com)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "choice-order"
    },
    {
      "cell_type": "code",
      "source": [
        "#df = loadTexts(workspace,datasetName)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "nasty-farmer"
    },
    {
      "cell_type": "code",
      "source": [
        "df_f = performNLP(workspace,minCount,mindf,minngram,maxngram,deleteEmptyTokens,df)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "hungarian-graphic"
    },
    {
      "cell_type": "code",
      "source": [
        "df_f.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "neural-lloyd"
    },
    {
      "cell_type": "code",
      "source": [
        "exportResults(workspace,datastore,fileName,df_f)\n",
        "\n",
        "#run.complete()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "lines_to_next_cell": 2
      },
      "id": "collectible-tulsa"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# coding: utf-8",
      "executable": "/usr/bin/env python",
      "notebook_metadata_filter": "-all",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent"
      }
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}