{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "qualified-english",
   "metadata": {},
   "source": [
    "<b>Import necessery libraries</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advised-contact",
   "metadata": {
    "gather": {
     "logged": 1639565695938
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace\n",
    "from azureml.data import DataType\n",
    "from spacy.cli.download import download as spacy_download\n",
    "import os \n",
    "from os.path import join as osjoin\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-timing",
   "metadata": {},
   "source": [
    "<b>Loading the Greek language tools</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "clear-segment",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('el_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "spacy_download('el_core_news_sm')\n",
    "nlp =spacy.load('el_core_news_sm', disable=['tagger', 'parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thorough-basics",
   "metadata": {},
   "source": [
    "<b>Regular expressions definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "greenhouse-chicago",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "p1 = re.compile('δεν απαντ.{1,3}\\s{0,1}',re.IGNORECASE)\n",
    "p2 = re.compile('\\sδα\\s',re.IGNORECASE)\n",
    "p3 = re.compile('δε.{0,1}\\s.{0,3}\\s{0,1}βρ.{1,2}κ.\\s{0,1}',re.IGNORECASE)\n",
    "p4 = re.compile('[^\\d]?\\d{10}')\n",
    "p5 = re.compile('[^\\d]?\\d{18}|[^\\d]\\d{20}')\n",
    "p6 = re.compile('δε[ ν]{0,1} (επιθυμ[α-ω]{2,4}?|[ήη]θ[εέ]λ[α-ω]{1,3}?|θελ[α-ω]{1,4}|.{0,20}ενδιαφ[εέ]ρ[α-ω]{2,4})',re.IGNORECASE)\n",
    "p7 = re.compile('δε[ ν]{0,1} (μπορ[α-ω]{2,5}|.εχει)',re.IGNORECASE)\n",
    "p8 = re.compile('(δεν|μη).*διαθεσιμ[οη]ς{0,1}?',re.IGNORECASE)\n",
    "p9 = re.compile('(δεν|μη)+.*εφικτη?',re.IGNORECASE)\n",
    "p10 = re.compile('δε[ ν]{0,1}.{0,20}θετικ[οόήη]ς{0,1}',re.IGNORECASE)\n",
    "p11 = re.compile('δε[ ν]{0,1}\\s?(γνωρ[α-ω|ά-ώ]{1,8}|ξ[εέ]ρ[α-ω|ά-ώ]{1,4}|απ[αά]ντ[α-ω|ά-ώ]{1,4})',re.IGNORECASE)\n",
    "\n",
    "p12 = re.compile('εξυπη.?(.*)',re.IGNORECASE)\n",
    "p13 = re.compile('τηλ[εέ]φ.?(.*)',re.IGNORECASE)\n",
    "p14 = re.compile('.*πρ(ο|ό)σ[ωώ]π.?(.*)',re.IGNORECASE)\n",
    "p15 = re.compile('αναμον.?(.*)',re.IGNORECASE)\n",
    "p16 = re.compile('χρ[οό]ν.?(.*)',re.IGNORECASE)\n",
    "p17 = re.compile('εμβ.?(.*)',re.IGNORECASE)\n",
    "p18 = re.compile('υπ[αά]λλ.?(.*)',re.IGNORECASE)\n",
    "p19 = re.compile('(υπο){0,1}καταστ.?(.*)', re.IGNORECASE)\n",
    "p20 = re.compile('πιστωτ.?(.*)', re.IGNORECASE)\n",
    "p21 = re.compile('διαδικα.?(.*)', re.IGNORECASE)\n",
    "p22 = re.compile('φωνητι.?(.*)', re.IGNORECASE)\n",
    "p23 = re.compile('γιο[υύ]ρο.?(.*)', re.IGNORECASE)\n",
    "p24 = re.compile('υπηρεσ.?(.*)', re.IGNORECASE)\n",
    "p25 = re.compile('κατ[αά]ρτι.?(.*)', re.IGNORECASE)\n",
    "p26 = re.compile('ανταπ.?(.*)', re.IGNORECASE)\n",
    "p27 = re.compile('υπηρ[εέ]σ.?(.*)', re.IGNORECASE)\n",
    "p28 = re.compile('πρ.βλ.μ.(τα){0,1}', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e683ce9c",
   "metadata": {},
   "source": [
    "#### Dictionary correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beae1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "corDict = {\n",
    "    **dict.fromkeys(['αμεσοτητα', 'καλυτερη', 'ταχυτερη', 'μεταφορα', 'κλησης', 'επικοινωνια', 'μεταφορα','τηλεφωνο', \n",
    "                   'εκπροσωπος', 'αμεση', 'γρηγορη', 'εξυπηρετηση', 'ουρα', 'γρηγοροτερη', 'καθυστερηση', 'προσωπικο',\n",
    "                    'ανταποκριση', 'μεγαλυτερη', 'περισσοτερη', 'καταστημα', 'ταχυτητα', 'οργανωση', 'αμεσοι', \n",
    "                     'γρηγοροι', 'ευελικτοι', 'ευκολοι', 'λαθος', 'λανθασμενη', 'ηλεκτρονικο', \n",
    "                     'ηλεκτρονικα', 'περιμενε', 'λειτουργιες', 'οδηγιες', 'επικοινωνιας'], 'εξυπηρετηση'), \n",
    "    **dict.fromkeys(['διαρκεια', 'χρονος', 'αρχικη', 'αναμονη', 'αντιμετωπιση', 'αναμονης'], 'αναμονη'),\n",
    "    **dict.fromkeys(['φωνητικη', 'πυλη', 'φωνητικη πυλη', 'συστημα', 'ηχητικες', 'ηχητικα'], 'φωνητικη πυλη'),\n",
    "    **dict.fromkeys(['καρτα', 'εμβασμα', 'δανεια', 'μετοχων', 'λογαριασμους', 'λογαριασμο'], 'προιοντα'),\n",
    "    **dict.fromkeys(['eBanking', 'europhonebanking', 'phonebanking', 'telephonebanking', 'europhone banking', \n",
    "                     'telephone banking','υπηρεσιες', 'telephone', 'banking', 'eurobank', 'mobile', 'euro',\n",
    "                    'εφαρμογη', 'εφαρμογης', 'πληρωμη', 'πληρωμης'], 'υπηρεσιες'),\n",
    "    **dict.fromkeys(['υπαλληλος', 'συμπεριφορα', 'συμπεριφορα υπαλληλου', 'καταρτιση'], 'υπαλληλοι'),\n",
    "    **dict.fromkeys(['προβλημα', 'υπαρχει', 'μεσω', 'τραπεζα','μπορουμε','πραγματα','πελατες','μπορω','δυο','ξερω','δευτερο',\n",
    "                     'δεκα', 'γιατι' ], ''),\n",
    "    **dict.fromkeys(['γραφειοκρατια', 'διαδικασιες'], 'διαδικασιες'),\n",
    "    **dict.fromkeys(['δενγνωριζωδεναπαντω', 'τιποτα'], 'δεν γνωριζω/δεν απαντω'),\n",
    "    **dict.fromkeys(['ολα', 'καλα', 'ενταξει', 'τελεια', 'ενταξει'], 'ευχαριστημενος'),\n",
    "    **dict.fromkeys(['κατι', 'αλλο'], 'αλλο')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-bobby",
   "metadata": {},
   "source": [
    "<b>Functions definitions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "documentary-nelson",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def replaceTerm(text):\n",
    "    \n",
    "    '''This function uses the above defined regular expressions to replace text\n",
    "    This function is applied before the accent mark removal\n",
    "    The order of the rules is important\n",
    "    Combinations of two or more words, are concatenated, in order to be considered as a single token'''\n",
    "\n",
    "    text = text.replace('γιουροφοουν','europhone')\n",
    "    text = text.replace('ιμπανκινγκ','ebanking')\n",
    "    text = text.replace('μπανκινγκ','banking')\n",
    "    text = text.replace('τελεφοουν','telephone')\n",
    "    text = text.replace('γιουρομπανκ','eurobank')\n",
    "    text = text.replace('γιουρο μπανκ','eurobank')\n",
    "    text = text.replace('βαιμπερ','viber')\n",
    "    text = text.replace('ιμαιλ', 'email')\n",
    "    text = text.replace('γιουζερνεημ', 'username')\n",
    "    text = text.replace('εητιεμ', 'ATM')\n",
    "    text = text.replace('φοουν', 'phone')\n",
    "    text = text.replace('γιουρο', 'euro')\n",
    "    text = text.replace('μομπαηλ', 'mobile')\n",
    "    \n",
    "    text = p5.sub(' λογαριασμος ',text)\n",
    "    text = p4.sub(' τηλεφωνο ',text)\n",
    "    text = p6.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p10.sub(' δενθελειδενενδιαφερεται ',text)\n",
    "    text = p7.sub(' δενεχειδενμπορει ',text)\n",
    "    text = p8.sub(' δενειναιδιαθεσιμος ',text)\n",
    "    text = p9.sub(' ανεφικτη ',text)\n",
    "    text = text.replace('-banking','banking')\n",
    "    text = text.replace('v banking','vbanking')\n",
    "    text = text.replace('e banking','ebanking')\n",
    "    text = text.replace('follow up','followup')\n",
    "    text = text.replace('fup','followup')\n",
    "    text = text.replace('f/up','followup')\n",
    "    text = text.replace('πυρ/ριο','πυρασφαλιστηριο')\n",
    "    text = text.replace('safe drive','safedrive')\n",
    "    text = text.replace('safe pocket','safepocket')\n",
    "    text = text.replace('alphabank','alpha')\n",
    "    text = text.replace('sweet home smart','sweethomesmart')\n",
    "    text = text.replace('sweet home','sweethome')\n",
    "    text = text.replace('eξασφαλιζω','εξασφαλιζω')\n",
    "    text = text.replace('credit card','creditcard')\n",
    "    text = text.replace('debit card','debitcard')\n",
    "    text = text.replace('life cycle','lifecycle')\n",
    "    text = text.replace('π/κ','πκ')\n",
    "    text = text.replace('td','πκ')\n",
    "    text = text.replace('α/κ','ακ')\n",
    "    text = text.replace('δ/α','δεναπαντα ')\n",
    "    text = text.replace('εκτος αττικης','εκτοςαττικης ')\n",
    "    #τδ\n",
    "    text = p1.sub(' δεναπαντα ',text)\n",
    "    text = p2.sub(' δεναπαντα ',text)\n",
    "    text = p3.sub(' δεντονβρηκα ',text)\n",
    "    text = p11.sub('δενγνωριζωδεναπαντω', text)\n",
    "    text = p12.sub('εξυπηρετηση', text)\n",
    "    text = p13.sub('τηλεφωνο', text)\n",
    "    text = p14.sub('εκπροσωπος', text)\n",
    "    text = p15.sub('αναμονης', text)\n",
    "    text = p16.sub('χρονος', text)\n",
    "    text = p17.sub('εμβασμα', text)\n",
    "    text = p18.sub('υπαλληλος', text)\n",
    "    text = p19.sub('καταστημα', text)\n",
    "    text = p20.sub('καρτα', text)\n",
    "    text = p21.sub('διαδικασιες', text)\n",
    "    text = p22.sub('φωνητικη', text)\n",
    "    text = p23.sub('euro', text)\n",
    "    text = p24.sub('υπηρεσιες', text)\n",
    "    text = p25.sub('καταρτιση', text)\n",
    "    text = p26.sub('ανταποκριση', text)\n",
    "    text = p27.sub('υπηρεσιες', text)\n",
    "    text = p28.sub('εξυπηρετηση', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "static-crossing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accent_mark(text):\n",
    "    \n",
    "    '''removes punctuation, removal of accent mark'''\n",
    "    \n",
    "    diction = {'ά':'α','έ':'ε','ί':'ι','ό':'ο','ώ':'ω','ύ':'υ','ή':'η'}\n",
    "    for key in diction.keys():\n",
    "        text = text.replace(key, diction[key])\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "spoken-novel",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def load_correctDict(ws):\\n    \\n    It creates a dictionary out of a dataset that containes pairs of (original term, corrected term)\\n    \\n    dataset = Dataset.get_by_name(ws, name=\\'correct_Tokens\\')    \\n    corDict = dict(dataset.to_pandas_dataframe().to_dict(\"split\")[\\'data\\'])\\n    return corDict'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def load_correctDict(ws):\n",
    "    \n",
    "    ''''''It creates a dictionary out of a dataset that containes pairs of (original term, corrected term)''''''\n",
    "    \n",
    "    dataset = Dataset.get_by_name(ws, name='correct_Tokens')    \n",
    "    corDict = dict(dataset.to_pandas_dataframe().to_dict(\"split\")['data'])\n",
    "    return corDict'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "passing-suite",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def correct(x,corDict):\n",
    "    \n",
    "    '''Uses the dictionary to correct the terms'''\n",
    "    \n",
    "    if x in corDict.keys():\n",
    "        y = corDict[x]\n",
    "    else:\n",
    "        y = x\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d978b75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokens(df, idField, textField):\n",
    "    \n",
    "    '''The variable columns is a list. The explode method \"unpivots\" this list'''\n",
    "    \n",
    "    # load stop words for the cleaning of the text --\"WORKSPACE\" IS READ AUTOMATICALLY\n",
    "    sw = loadStopWords()\n",
    "    \n",
    "    df = df[[idField,textField]]\n",
    "    df['tokenized'] = df[textField].apply(clean_text, sw =sw)\n",
    "    \n",
    "    df = df.fillna('N/A')\n",
    "    \n",
    "    df['variable'] = df['tokenized'].str.split()\n",
    "    df_f = df.explode('variable')[[idField, 'tokenized','variable']]\n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "permanent-tolerance",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def loadStopWords():\n",
    "\n",
    "    '''A dataset containing the Greek stop words has been created \n",
    "    the function loads this dataset as a dataframe'''\n",
    "    \n",
    "    dataset = Dataset.get_by_name(workspace, name='stopWords_gr')\n",
    "    sw = set(dataset.to_pandas_dataframe().squeeze())\n",
    "    return sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "painful-dairy",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def clean_text(text, sw):\n",
    "    '''This function performs text cleansing and returns the clean and lemmatized version of the original text\n",
    "    convert to lower text '''\n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # remove puncutation\n",
    "    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n",
    "    \n",
    "    # remove accent mark\n",
    "    text = [remove_accent_mark(x) for x in text]\n",
    "    \n",
    "    #replacements either by rules or regular expressions\n",
    "    text = [replaceTerm(x) for x in text]\n",
    "\n",
    "    # remove stop words\n",
    "    text = [x for x in text if x not in sw]\n",
    "\n",
    "    #remove quotes\n",
    "    text = [x.replace('quot;','').replace('&quot','') for x in text if x not in {'quot','amp'}]\n",
    "    \n",
    "    # remove words that contain numbers\n",
    "    text = [word for word in text if not any(c.isdigit() for c in word)] #addition to return even empty tokens\n",
    "    \n",
    "    # remove empty tokens\n",
    "    #text = [t for t in text if len(t) > 0] #addition to return even empty tokens\n",
    "    \n",
    "    # remove amp & quot\n",
    "    text = [x for x in text if x not in ['quot','amp']]\n",
    "    \n",
    "    # remove words with only one letter\n",
    "    text = \" \".join([t for t in text if len(t) > -1]) #addition to return even empty tokens\n",
    "    \n",
    "    # lemmatize text\n",
    "    text = \" \".join([t.lemma_ for t in nlp(text, disable=['tagger', 'parser', 'ner','tok2vec', 'morphologizer', 'parser', 'senter', 'attribute_ruler',  'ner'])])\n",
    "    \n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "personal-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTokencount(df_f,minCount):\n",
    "    \n",
    "    '''Calculate the number of occurances (counts) of each token\n",
    "    tokens with count less than mincount are set to blank'''\n",
    "\n",
    "    tokenCount = df_f['variable'].value_counts().to_dict()\n",
    "    \n",
    "    df_f['value'] = df_f['variable'].map(tokenCount)\n",
    "   \n",
    "    df_f.loc[(df_f['value'] < minCount), 'variable'] = ' ' #addition to return even empty tokens\n",
    "    \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "global-chess",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_ngrams(idf,mindf,minngram,maxngram,idField):\n",
    "    \n",
    "    '''This function returns the bi-grams and tri-grams'''\n",
    "    idf = idf.reset_index(drop = True)\n",
    "    tfidf = TfidfVectorizer(min_df = mindf, ngram_range = (minngram,maxngram))\n",
    "    tfidf_df = pd.DataFrame(tfidf.fit_transform(idf['tokenized']).toarray(), columns = tfidf.get_feature_names())\n",
    "\n",
    "    df_i = pd.concat([idf[[idField]],tfidf_df],axis=1).melt(id_vars=[idField],value_vars = tfidf_df.columns).dropna()\n",
    "    df_i = df_i[df_i['value'] > 0].reset_index(drop=True)\n",
    "    return df_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bizarre-finger",
   "metadata": {},
   "outputs": [],
   "source": [
    "def performNLP(minCount, ngram_param, df, idField, textField, min_importance = 0.7, \n",
    "               corDict = None, deleteEmptyTokens = True):\n",
    "    \n",
    "    '''Loads all above functions for the cleaning of the text and the extraction of bigrams and trigrams'''\n",
    "       \n",
    "    df_f = getTokens(df, idField,textField)\n",
    "    \n",
    "    df_f = df_f.fillna(' ')\n",
    "    \n",
    "    df_f = getTokencount(df_f, minCount)\n",
    "    \n",
    "    try:\n",
    "        df_f = df_f.append(get_ngrams(df_f, ngram_param[0], ngram_param[1], ngram_param[2], idField))\n",
    "    except:\n",
    "        print('no bigramms or trigramms were added')  \n",
    "    \n",
    "    df_f = df_f.loc[df_f['value'] > min_importance]\n",
    "    \n",
    "    df_f['token'] = df_f['variable']\n",
    "     \n",
    "    df_f.loc[(df_f['token'].str.len() <= 5), 'token'] = ' ' #single of double character tokens are set to blank\n",
    "    \n",
    "    df_f = df_f.sort_values([idField,'token'])\n",
    "    \n",
    "    if corDict != None: df_f['token'] = df_f['variable'].map(corDict).fillna('')\n",
    "        \n",
    "    if deleteEmptyTokens:\n",
    "        df_f = df_f[df_f['token'] != ' ']\n",
    "    \n",
    "    df_f = df_f[[idField, 'token']].drop_duplicates().reset_index(drop=True)\n",
    "    \n",
    "    return df_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "synthetic-density",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTexts(datasetName,idField,textField):\n",
    "    \n",
    "    '''loads the texts to be analyzed'''\n",
    "    \n",
    "    dataset = Dataset.get_by_name(workspace, name=datasetName)\n",
    "    df = dataset.to_pandas_dataframe()\n",
    "    df= df[[idField,textField]]\n",
    "    return df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "intimate-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exportResults(fileName,df_f):\n",
    "    \n",
    "    '''Export results to a .txt file'''\n",
    "    \n",
    "    df_f.to_csv(fileName+'.txt',sep =',',line_terminator='\\r\\n',index = False)\n",
    "    fil = [os.getcwd()+'/'+ fileName+'.txt']\n",
    "    #datastore.upload_files(fil, target_path='UI/NLP', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a379642",
   "metadata": {},
   "source": [
    "### Parameter definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8852d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '6ed9d167-b2e6-41b8-9500-35e6df64d9dc'\n",
    "resource_group = 'MLRG'\n",
    "workspace_name = 'erbbimlws'\n",
    "\n",
    "global workspace \n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "#minimum number of tokens in the texts\n",
    "minCount = 1\n",
    "#ngrams parameters\n",
    "ngram_param = [3,2,2]\n",
    "fileName = 'Omilia_Vana'\n",
    "\n",
    "idField = 'id'\n",
    "textField = 'NPS_UTTERANCE_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "nasty-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read manually an excel sheet with the comments to be analyzed\n",
    "#df = pd.read_excel('./xlsxFiles/NPS_CSAT_0102_2022.xlsx',engine='openpyxl',index_col = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hungarian-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call perform NLP for the NLP manipulation\n",
    "df_f = performNLP(minCount,ngram_param,df,idField,textField, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "collectible-tulsa",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "exportResults(fileName,df_f)\n",
    "#run.complete()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "notebook_metadata_filter": "-all"
  },
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
